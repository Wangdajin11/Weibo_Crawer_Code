import requests
import json
from datetime import datetime, timedelta
import csv
import time
import os
import logging
import threading
from queue import Queue
import schedule
import random
import re
import pandas as pd
import glob
import getpass
from urllib.parse import quote
import traceback

# æ–°å¢å¯¼å…¥
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('weibo_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ================== å·¥å…·å‡½æ•° ==================
def extract_user_id(url):
    """ä»ç”¨æˆ·URLä¸­æå–ç”¨æˆ·ID - å¢å¼ºç‰ˆ"""
    if not url:
        return ""
    
    # å¤„ç†å„ç§å¾®åšç”¨æˆ·URLæ ¼å¼
    patterns = [
        r'(?:/u/|/profile/|^/|^https?://weibo\.com/)(\d+)(?:\?.*)?$',
        r'^https?://(?:www\.)?weibo\.com/p/100(\d{9})$',
        r'^https?://(?:www\.)?weibo\.com/(\d+)/?.*$',
        r'uid=(\d+)',  # å¤„ç†å«uidå‚æ•°çš„é“¾æ¥
        r'weibo\.com/(\d+)\?',  # å¤„ç†å¸¦å‚æ•°çš„é“¾æ¥
        r'weibo\.com/u/(\d+)',  # å¤„ç†/u/æ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url or "")
        if match:
            user_id = match.group(1)
            # å¦‚æœæ˜¯100å¼€å¤´çš„ç‰¹æ®Šæ ¼å¼ï¼Œéœ€è¦å¤„ç†
            if pattern == r'^https?://(?:www\.)?weibo\.com/p/100(\d{9})$':
                user_id = "100" + user_id
            return user_id
    return ""

def sanitize_filename(keyword):
    """æ¸…ç†éæ³•æ–‡ä»¶åå­—ç¬¦"""
    return re.sub(r'[\\/*?:"<>|]', '_', keyword)[:120]

def parse_interaction_count(text):
    """è§£æäº’åŠ¨æ•°å­—ï¼ˆä¸‡å•ä½å¤„ç†ï¼‰"""
    if not text:
        return 0
    match = re.search(r'(\d+\.?\d*)(ä¸‡?)', text)
    if match:
        num = float(match.group(1))
        return int(num * 10000) if match.group(2) == 'ä¸‡' else int(num)
    return 0

def parse_weibo_time(text):
    """æ™ºèƒ½è§£æå¾®åšæ—¶é—´æ ¼å¼"""
    if not text:
        return "", ""
        
    now = datetime.now()
    clean_text = text.strip().replace("\u200b", "").replace("\xa0", " ").replace("\n", "")
    
    time_units = {
        "åˆšåˆš": (0, 'seconds'),
        "ç§’å‰": ('seconds', 1),
        "åˆ†é’Ÿå‰": ('minutes', 1),
        "å°æ—¶å‰": ('hours', 1),
        "å¤©å‰": ('days', 1),
        "æ˜¨å¤©": ('days', 1)
    }
    
    for pattern, (unit, mult) in time_units.items():
        if pattern in clean_text:
            if pattern == "æ˜¨å¤©":
                return (now - timedelta(days=1)).strftime('%Y-%m-%d') + " " + clean_text.split()[-1], text
            num = int(re.search(r'\d+', clean_text).group()) if pattern != "åˆšåˆš" else 0
            delta = {unit: num * mult} if unit else {}
            return (now - timedelta(**delta)).strftime('%Y-%m-%d %H:%M:%S'), text

    try:
        dt = datetime.strptime(clean_text, "%Yå¹´%mæœˆ%dæ—¥ %H:%M")
        return dt.strftime('%Y-%m-%d %H:%M:%S'), text
    except:
        pass
    
    try:
        dt = datetime.strptime(clean_text, "%mæœˆ%dæ—¥ %H:%M").replace(year=now.year)
        return dt.strftime('%Y-%m-%d %H:%M:%S'), text
    except:
        pass
    
    return clean_text, text

def generate_date_range(start_date, end_date):
    """ç”Ÿæˆæ—¥æœŸèŒƒå›´"""
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    date_list = []
    
    current = start
    while current <= end:
        date_list.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)
    
    return date_list

class WeiboAdvancedSearchCrawler:
    def __init__(self, config):
        self.session = requests.Session()
        self.config = config
        
        # ä½¿ç”¨Seleniumç™»å½•è·å–Cookie
        self.driver = self.setup_browser()
        self.header = self.login_with_selenium()
        
        # æ•°æ®ä¿å­˜è·¯å¾„
        self.base_path = r""   #ä¿®æ”¹ä¸ºè‡ªå·±çš„ä¿å­˜è·¯å¾„
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = Queue()
        self.crawled_mids = set()
        self.is_running = True
        
        # æ•°æ®å­˜å‚¨ç»“æ„
        # æŒ‰å…³é”®è¯å’Œå¹´ä»½ç»„ç»‡æ•°æ®
        self.keyword_data = {}
        self.total_weibo_count = 0
        
        # åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
        self.create_directories()
        
        # åˆå§‹åŒ–æ•°æ®ç»“æ„
        self.init_data_structure()
    
    def init_data_structure(self):
        """åˆå§‹åŒ–æ•°æ®ç»“æ„ - æŒ‰å…³é”®è¯å’Œå¹´ä»½ç»„ç»‡"""
        for keyword in self.config['keywords']:
            self.keyword_data[keyword] = {}
            # ä¸ºæ¯ä¸ªå…³é”®è¯åˆå§‹åŒ–å¹´ä»½æ•°æ®å­˜å‚¨
            start_year = int(self.config['start_date'][:4])
            end_year = int(self.config['end_date'][:4])
            for year in range(start_year, end_year + 1):
                self.keyword_data[keyword][str(year)] = []
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        options = Options()
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        return driver
    
    def login_with_selenium(self):
        """ä½¿ç”¨Seleniumè‡ªåŠ¨åŒ–ç™»å½•è·å–Cookie"""
        print("\n" + "="*80)
        print("å¯åŠ¨å¾®åšè‡ªåŠ¨åŒ–ç™»å½•...")
        print("="*80)
        
        try:
            self.driver.get("https://weibo.com/login.php")
            
            print("è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆå¾®åšç™»å½•...")
            print("æç¤ºï¼š")
            print("   - å¦‚æœå‡ºç°æ‰«ç ç™»å½•ï¼Œè¯·ä½¿ç”¨å¾®åšAPPæ‰«ç ")
            print("   - å¦‚æœä½¿ç”¨è´¦å·å¯†ç ç™»å½•ï¼Œè¯·ç¡®ä¿è´¦å·æ­£ç¡®")
            print("   - ç™»å½•æˆåŠŸåï¼Œè¯·åœ¨æ§åˆ¶å°æŒ‰å›è½¦ç»§ç»­")
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•åæŒ‰å›è½¦
            input("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç™»å½•æˆåŠŸåæŒ‰å›è½¦ç»§ç»­...")
            
            # è·å–Cookieå¹¶æ„å»ºè¯·æ±‚å¤´
            cookies = self.driver.get_cookies()
            cookie_str = '; '.join([f"{c['name']}={c['value']}" for c in cookies])
            
            # æ„å»ºè¯·æ±‚å¤´
            header = {
                'Cookie': cookie_str,
                'User-Agent': self.driver.execute_script("return navigator.userAgent"),
                'Referer': 'https://weibo.com/',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'X-Requested-With': 'XMLHttpRequest',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin'
            }
            
            # å°è¯•æå–XSRF-TOKEN
            xsrf_match = re.search(r'XSRF-TOKEN=([^;]+)', cookie_str)
            if xsrf_match:
                header['X-XSRF-TOKEN'] = xsrf_match.group(1)
                print(f"æå–åˆ°XSRF-TOKEN: {xsrf_match.group(1)[:10]}...")
            
            print(f"æˆåŠŸè·å–Cookieï¼ˆé•¿åº¦ï¼š{len(cookie_str)}å­—ç¬¦ï¼‰")
            
            # æµ‹è¯•Cookieæœ‰æ•ˆæ€§
            if self.test_cookie_effectiveness(header):
                print("Cookieæœ‰æ•ˆæ€§æµ‹è¯•é€šè¿‡ï¼")
            else:
                print("Cookieæµ‹è¯•æœªé€šè¿‡ï¼Œä½†ç»§ç»­è¿è¡Œ...")
            
            return header
            
        except Exception as e:
            print(f"è‡ªåŠ¨åŒ–ç™»å½•å¤±è´¥: {e}")
            traceback.print_exc()
            raise
    
    def smart_scroll(self):
        """ä¼˜åŒ–åçš„æ™ºèƒ½æ»šåŠ¨åŠ è½½"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        retry = 0
        
        while retry < 5:  # SCROLL_RETRIES = 5
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1.0, 2.0))  # ä¼˜åŒ–ç­‰å¾…æ—¶é—´
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                retry += 1
                time.sleep(1)
            else:
                retry = 0
                last_height = new_height

    def test_cookie_effectiveness(self, headers):
        """æµ‹è¯•Cookieæ˜¯å¦æœ‰æ•ˆ"""
        try:
            print("\nğŸ”§ æ­£åœ¨æµ‹è¯•Cookieæœ‰æ•ˆæ€§...")
            
            # æµ‹è¯•åŸºç¡€API
            url = "https://weibo.com/ajax/profile/info?uid=1669879400"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                if response.text.startswith('<!DOCTYPE') or response.text.startswith('<html'):
                    print("è¿”å›äº†HTMLç™»å½•é¡µé¢ï¼ŒCookieå¯èƒ½æ— æ•ˆ")
                    return False
                
                try:
                    data = response.json()
                    if data.get('ok'):
                        print("åŸºç¡€APIæµ‹è¯•é€šè¿‡")
                        return True
                    else:
                        print(f"APIè¿”å›é”™è¯¯: {data.get('msg', 'Unknown error')}")
                        return False
                except json.JSONDecodeError:
                    print("è¿”å›çš„ä¸æ˜¯JSONæ ¼å¼")
                    return False
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
                
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def create_directories(self):
        """åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•"""
        try:
            os.makedirs(os.path.join(self.base_path, 'WeiboData'), exist_ok=True)
            logging.info(f"æ•°æ®å°†ä¿å­˜åˆ°: {self.base_path}")
        except Exception as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            raise
    
    def save_weibo_data(self, weibo_list, keyword, date):
        """ä¿å­˜å¾®åšæ•°æ®åˆ°å†…å­˜ç»“æ„ï¼ŒæŒ‰å…³é”®è¯å’Œå¹´ä»½ç»„ç»‡"""
        if not weibo_list:
            return
        
        saved_count = 0
        for data in weibo_list:
            if data and data.get('mid') not in self.crawled_mids:
                # æå–å¹´ä»½
                pub_time = data.get('å‘å¸ƒæ—¶é—´', '')
                if pub_time and len(pub_time) >= 4:
                    year = pub_time[:4]
                else:
                    # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨æœç´¢æ—¥æœŸçš„å¹´ä»½
                    year = date[:4]
                
                # åŠ¨æ€åˆ›å»ºæ•°æ®ç»“æ„ï¼ˆé˜²æ­¢å¹´ä»½ä¸åœ¨åˆå§‹åŒ–èŒƒå›´å†…ï¼‰
                if keyword not in self.keyword_data:
                    self.keyword_data[keyword] = {}
                if year not in self.keyword_data[keyword]:
                    self.keyword_data[keyword][year] = []
                
                # å°†æ•°æ®æ·»åŠ åˆ°å¯¹åº”çš„å…³é”®è¯å’Œå¹´ä»½
                self.keyword_data[keyword][year].append(data)
                if data.get('mid'):
                    self.crawled_mids.add(data['mid'])
                self.total_weibo_count += 1
                saved_count += 1
        
        if saved_count > 0:
            logging.info(f"æˆåŠŸä¿å­˜äº† {saved_count} æ¡æ•°æ® | æ€»è®¡: {self.total_weibo_count} æ¡")
            
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆè€Œä¸æ˜¯ç­‰å¾…100æ¡ï¼‰
            self.save_to_excel_files()
        else:
            logging.info(f"æ²¡æœ‰æ–°æ•°æ®ä¿å­˜ï¼Œå¯èƒ½éƒ½æ˜¯é‡å¤æ•°æ®")
    
    def save_to_excel_files(self):
        """å°†å†…å­˜ä¸­çš„æ•°æ®ä¿å­˜åˆ°Excelæ–‡ä»¶ - æ¯ä¸ªå…³é”®è¯ä¸€ä¸ªæ–‡ä»¶ï¼Œæ¯å¹´ä¸€ä¸ªå·¥ä½œè¡¨"""
        if self.total_weibo_count == 0:
            logging.info("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
            
        for keyword, year_data in self.keyword_data.items():
            if not any(year_data.values()):  # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡
                continue
                
            # åˆ›å»ºæ–‡ä»¶å
            filename = f"{sanitize_filename(keyword)}_å¾®åšæ•°æ®.xlsx"
            filepath = os.path.join(self.base_path, 'WeiboData', filename)
            
            try:
                # ä½¿ç”¨ExcelWriteræ¥ä¿å­˜å¤šä¸ªå·¥ä½œè¡¨
                with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
                    for year, data_list in year_data.items():
                        if data_list:  # åªä¿å­˜æœ‰æ•°æ®çš„å¹´ä»½
                            # è½¬æ¢ä¸ºDataFrame
                            df = pd.DataFrame(data_list)
                            
                            # ä¿å­˜åˆ°å¯¹åº”çš„å·¥ä½œè¡¨
                            sheet_name = f"{year}å¹´"
                            # Excelå·¥ä½œè¡¨åç§°é™åˆ¶åœ¨31ä¸ªå­—ç¬¦ä»¥å†…
                            if len(sheet_name) > 31:
                                sheet_name = sheet_name[:31]
                            
                            df.to_excel(writer, sheet_name=sheet_name, index=False)
                            
                            logging.info(f"ä¿å­˜å…³é”®è¯ '{keyword}' {year}å¹´æ•°æ®: {len(data_list)} æ¡")
                
                logging.info(f"å…³é”®è¯ '{keyword}' æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
                
            except Exception as e:
                logging.error(f"ä¿å­˜å…³é”®è¯ '{keyword}' æ•°æ®å¤±è´¥: {e}")
                traceback.print_exc()
    
    def final_save(self):
        """æœ€ç»ˆä¿å­˜ - çˆ¬å–å®Œæˆåè°ƒç”¨"""
        logging.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ•°æ®ä¿å­˜...")
        self.save_to_excel_files()
        if self.total_weibo_count > 0:
            logging.info(f"æ‰€æœ‰æ•°æ®ä¿å­˜å®Œæˆï¼æ€»è®¡: {self.total_weibo_count} æ¡å¾®åš")
        else:
            logging.info("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
    
    def _get_headers(self):
        """ç”Ÿæˆè¯·æ±‚å¤´ - å‚è€ƒæä¾›çš„ä»£ç """
        cookies = {c['name']: c['value'] for c in self.driver.get_cookies()}
        return {
            'user-agent': self.driver.execute_script("return navigator.userAgent"),
            'referer': 'https://weibo.com/',
            'x-requested-with': 'XMLHttpRequest',
            'cookie': '; '.join([f"{k}={v}" for k, v in cookies.items()])
        }
    
    def get_comments(self, mid):
        """ä¿®å¤è¯„è®ºè·å–åŠŸèƒ½ - å®Œå…¨å‚è€ƒæä¾›çš„ä»£ç """
        if not mid or not mid.isdigit():
            return ""
        
        comments = []
        max_id = None
        
        for _ in range(5):  # å‡å°‘è¯„è®ºè·å–æ·±åº¦
            try:
                params = {
                    'is_reload': 1,
                    'id': mid,
                    'is_show_bulletin': 2,
                    'count': 20,
                    'max_id': max_id or 0
                }
                
                # ä½¿ç”¨å‚è€ƒä»£ç çš„headersæ ¼å¼
                headers = self._get_headers()
                
                response = requests.get(
                    'https://weibo.com/ajax/statuses/buildComments',
                    params=params,
                    headers=headers,
                    timeout=15
                )
                
                if response.status_code == 200:
                    data = response.json()
                    # æå–è¯„è®ºå†…å®¹ - ä½¿ç”¨text_rawå­—æ®µ
                    comments.extend([c.get('text_raw', '') for c in data.get('data', [])])
                    max_id = data.get('max_id', 0)
                    if max_id <= 0:
                        break
                time.sleep(random.uniform(1.5, 2.5))
                    
            except Exception as e:
                logging.warning(f"è·å–è¯„è®ºå¤±è´¥: {str(e)}")
                break
        
        return '||'.join(filter(None, comments))
    
    def parse_weibo_card(self, card, keyword, search_date):
        """ä¼˜åŒ–è§£æé€»è¾‘ - å¢å¼ºå®¹é”™æ€§ï¼Œç‰¹åˆ«ä¿®å¤ç”¨æˆ·IDè·å–"""
        try:
            # è·å–MID - å°è¯•å¤šç§æ–¹å¼
            mid = ""
            try:
                # å°è¯•ä»cardå±æ€§è·å–
                mid = card.get('mid', '')
                if not mid:
                    # å°è¯•ä»mblogå±æ€§è·å–
                    mblog = card.get('mblog', {})
                    if mblog:
                        mid = mblog.get('mid', '')
                if not mid:
                    # å°è¯•ä»é“¾æ¥ä¸­æå–
                    mid_match = re.search(r'mid=(\d+)', str(card))
                    if mid_match:
                        mid = mid_match.group(1)
            except:
                pass

            # ç”¨æˆ·ä¿¡æ¯è§£æ - ä¿®å¤ç”¨æˆ·IDè·å–é—®é¢˜
            user = "æœªçŸ¥ç”¨æˆ·"
            user_id = ""
            try:
                # é¦–å…ˆå°è¯•è·å–ç”¨æˆ·é“¾æ¥å’Œç”¨æˆ·ä¿¡æ¯
                user_selectors = [
                    "a.name",
                    ".name",
                    ".username",
                    "a[href*='weibo.com']",
                    ".txt_attr"
                ]
                
                for selector in user_selectors:
                    user_tag = card.select_one(selector)
                    if user_tag and user_tag.get_text(strip=True):
                        user = user_tag.get_text(strip=True)
                        user_link = user_tag.get('href', '')
                        
                        # å¢å¼ºç”¨æˆ·IDæå–é€»è¾‘
                        if user_link:
                            user_id = extract_user_id(user_link)
                            if user_id:
                                break
                        
                        # å¦‚æœä»é“¾æ¥ä¸­æå–ä¸åˆ°ï¼Œå°è¯•ä»å…¶ä»–å±æ€§ä¸­æå–
                        if not user_id:
                            # å°è¯•ä»data-usercardå±æ€§ä¸­æå–
                            usercard = user_tag.get('usercard', '') or user_tag.get('data-usercard', '')
                            if usercard and 'id=' in usercard:
                                uid_match = re.search(r'id=(\d+)', usercard)
                                if uid_match:
                                    user_id = uid_match.group(1)
                                    break
                
                # å¦‚æœä»ç„¶æ²¡æœ‰è·å–åˆ°ç”¨æˆ·IDï¼Œå°è¯•ä»æ•´ä¸ªå¡ç‰‡ä¸­æœç´¢ç”¨æˆ·ID
                if not user_id:
                    # æœç´¢æ‰€æœ‰å¯èƒ½çš„ç”¨æˆ·IDæ¨¡å¼
                    card_text = str(card)
                    uid_patterns = [
                        r'weibo\.com/(\d+)(?:\?|")',
                        r'uid=(\d+)',
                        r'usercard.*?id=(\d+)',
                        r'/(\d+)/profile'
                    ]
                    
                    for pattern in uid_patterns:
                        uid_match = re.search(pattern, card_text)
                        if uid_match:
                            user_id = uid_match.group(1)
                            break
                            
            except Exception as e:
                logging.warning(f"ç”¨æˆ·ä¿¡æ¯è§£æå¤±è´¥: {e}")

            # å†…å®¹è§£æ - å¢å¼ºå®¹é”™æ€§
            content = ""
            try:
                content_selectors = [
                    "div[node-type='feed_list_content']",
                    ".txt",
                    ".weibo-text",
                    ".content",
                    "p"
                ]
                
                for selector in content_selectors:
                    content_tag = card.select_one(selector)
                    if content_tag:
                        content = content_tag.get_text(separator='', strip=True)
                        if content:
                            break
            except:
                pass

            # æ—¶é—´è§£æ - å¢å¼ºå®¹é”™æ€§
            pub_time = ""
            raw_time = ""
            try:
                time_selectors = [
                    "a[date]",
                    ".from",
                    ".time",
                    ".date"
                ]
                
                for selector in time_selectors:
                    time_tag = card.select_one(selector)
                    if time_tag:
                        time_text = time_tag.get_text(strip=True)
                        if time_text:
                            pub_time, raw_time = parse_weibo_time(time_text)
                            break
            except:
                pass

            # äº’åŠ¨æ•°æ®è§£æ - å¢å¼ºå®¹é”™æ€§
            reposts = 0
            comments = 0
            likes = 0
            
            try:
                # å°è¯•å¤šç§æ–¹å¼è·å–äº’åŠ¨æ•°æ®
                interaction_selectors = [
                    "div.card-act ul li",
                    ".card-act li",
                    ".action li",
                    ".toolbar span"
                ]
                
                for selector in interaction_selectors:
                    interactions = card.select(selector)
                    if interactions and len(interactions) >= 3:
                        reposts = parse_interaction_count(interactions[1].text) if len(interactions) > 1 else 0
                        comments = parse_interaction_count(interactions[2].text) if len(interactions) > 2 else 0
                        likes = parse_interaction_count(interactions[3].text) if len(interactions) > 3 else 0
                        break
            except:
                pass

            # å¦‚æœå…³é”®ä¿¡æ¯éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡è¿™æ¡å¾®åš
            if not content and not user and not pub_time:
                return None
            
            # è·å–è¯„è®º
            comment_text = ""
            if mid:
                try:
                    comment_text = self.get_comments(mid)
                    if comment_text:
                        logging.info(f"æˆåŠŸè·å–å¾®åš {mid} çš„è¯„è®ºï¼Œå…± {len(comment_text.split('||'))} æ¡")
                except Exception as e:
                    logging.warning(f"è·å–è¯„è®ºå¤±è´¥: {str(e)}")
            
            return {
                "mid": mid,
                "ç”¨æˆ·": user,
                "ç”¨æˆ·ID": user_id,
                "å†…å®¹": content,
                "å‘å¸ƒæ—¶é—´": pub_time,
                "åŸå§‹æ—¶é—´": raw_time,
                "è½¬å‘": reposts,
                "è¯„è®ºæ•°": comments,
                "ç‚¹èµ": likes,
                "è¯¦ç»†è¯„è®º": comment_text,
                "å…³é”®è¯": keyword,
                "æœç´¢æ—¥æœŸ": search_date
            }
            
        except Exception as e:
            logging.error(f"å¡ç‰‡è§£æå¤±è´¥: {str(e)}")
            return None
    
    def advanced_search_by_date(self, keyword, search_date, max_pages=50):
        """ä½¿ç”¨é«˜çº§æœç´¢æŒ‰æ—¥æœŸæœç´¢å¾®åš"""
        print(f"å¼€å§‹æœç´¢å…³é”®è¯ '{keyword}' åœ¨æ—¥æœŸ '{search_date}' çš„å¾®åš...")
        all_data = []
        
        # æ„é€ é«˜çº§æœç´¢URL
        encoded_keyword = quote(keyword)
        url = f"https://s.weibo.com/weibo?q={encoded_keyword}&typeall=1&suball=1&timescope=custom:{search_date}:{search_date}&page=1"
        
        for page in range(1, max_pages + 1):
            print(f"æ­£åœ¨é‡‡é›†ç¬¬ {page} é¡µ...")
            retry = 0
            
            while retry < 3:
                try:
                    # æ„é€ å¸¦é¡µç çš„URL
                    if page > 1:
                        page_url = f"https://s.weibo.com/weibo?q={encoded_keyword}&typeall=1&suball=1&timescope=custom:{search_date}:{search_date}&page={page}"
                    else:
                        page_url = url
                    
                    print(f"è®¿é—®URL: {page_url}")
                    self.driver.get(page_url)
                    
                    # ç­‰å¾…é¡µé¢åŠ è½½ - ä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶
                    WebDriverWait(self.driver, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.card"))
                    )
                    
                    # æ™ºèƒ½æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
                    self.smart_scroll()
                    
                    # è§£æé¡µé¢ - ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    
                    # å°è¯•å¤šç§å¡ç‰‡é€‰æ‹©å™¨
                    card_selectors = [
                        "div[action-type='feed_list_item']",
                        "div.card",
                        ".card",
                        ".wb-card"
                    ]
                    
                    cards = []
                    for selector in card_selectors:
                        cards = soup.select(selector)
                        if cards:
                            print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(cards)} å¼ å¡ç‰‡")
                            break
                    
                    if not cards:
                        print(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œç»ˆæ­¢é‡‡é›†")
                        return all_data
                    
                    page_data = []
                    for i, card in enumerate(cards):
                        print(f"è§£æç¬¬ {i+1} å¼ å¡ç‰‡...")
                        data = self.parse_weibo_card(card, keyword, search_date)
                        if data:
                            page_data.append(data)
                            print(f"æˆåŠŸè§£æå¡ç‰‡ {i+1} | ç”¨æˆ·: {data.get('ç”¨æˆ·', 'æœªçŸ¥')} | ç”¨æˆ·ID: {data.get('ç”¨æˆ·ID', 'æœªçŸ¥')}")
                        else:
                            print(f"å¡ç‰‡ {i+1} è§£æå¤±è´¥")
                    
                    if not page_data:
                        print(f"ç¬¬ {page} é¡µæ— æœ‰æ•ˆæ•°æ®ï¼Œç»ˆæ­¢é‡‡é›†")
                        return all_data
                    
                    all_data.extend(page_data)
                    self.save_weibo_data(page_data, keyword, search_date)
                    print(f"ç¬¬ {page} é¡µé‡‡é›†å®Œæˆï¼Œå…± {len(page_data)} æ¡æ•°æ®")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    next_page_selectors = [
                        "a.next",
                        ".next",
                        "a[class*='next']"
                    ]
                    
                    has_next = False
                    for selector in next_page_selectors:
                        next_page = soup.select_one(selector)
                        if next_page:
                            has_next = True
                            break
                    
                    if not has_next:
                        print("æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œç»ˆæ­¢é‡‡é›†")
                        return all_data
                    
                    time.sleep(random.uniform(3, 6))  # å¢åŠ é¡µé¢é—´éšæœºç­‰å¾…
                    break
                    
                except TimeoutException:
                    retry += 1
                    print(f"ç¬¬ {page} é¡µåŠ è½½è¶…æ—¶ï¼Œé‡è¯•ä¸­ ({retry}/3)")
                    # å°è¯•é‡æ–°åŠ è½½é¡µé¢
                    try:
                        self.driver.refresh()
                        time.sleep(5)
                    except:
                        pass
                except Exception as e:
                    print(f"ç¬¬ {page} é¡µé‡‡é›†å¼‚å¸¸: {str(e)}")
                    traceback.print_exc()
                    break
        
        return all_data
    
    def crawl_historical_data(self, keywords, start_date, end_date, max_pages_per_day=50):
        """çˆ¬å–å†å²æ•°æ® - æŒ‰å¤©çˆ¬å–"""
        date_range = generate_date_range(start_date, end_date)
        total_days = len(date_range)
        
        print(f"å¼€å§‹çˆ¬å–å†å²æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"æ€»å…± {total_days} å¤©ï¼Œå…³é”®è¯: {keywords}")
        print("=" * 60)
        
        for i, date in enumerate(date_range, 1):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i}/{total_days} å¤©: {date}")
            
            for keyword in keywords:
                try:
                    print(f"æœç´¢å…³é”®è¯: {keyword}")
                    data = self.advanced_search_by_date(keyword, date, max_pages_per_day)
                    
                    if data:
                        print(f"{date} å…³é”®è¯ '{keyword}' å…±è·å– {len(data)} æ¡æ•°æ®")
                    else:
                        print(f"{date} å…³é”®è¯ '{keyword}' æ— æ•°æ®")
                    
                    # å…³é”®è¯é—´å»¶æ—¶
                    time.sleep(random.uniform(5, 10))
                    
                except Exception as e:
                    print(f"{date} å…³é”®è¯ '{keyword}' é‡‡é›†å¤±è´¥: {str(e)}")
                    continue
            
            # æ¯å¤©çˆ¬å–å®Œæˆåå»¶æ—¶
            if i < total_days:
                delay = random.uniform(10, 20)
                print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­ä¸‹ä¸€æ—¥...")
                time.sleep(delay)
        
        # çˆ¬å–å®Œæˆåè¿›è¡Œæœ€ç»ˆä¿å­˜
        self.final_save()
    
    def start_continuous_crawling(self, config):
        """å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
        def crawl_task():
            try:
                # çˆ¬å–å†å²æ•°æ®
                if 'keywords' in config and 'start_date' in config and 'end_date' in config:
                    self.crawl_historical_data(
                        keywords=config['keywords'],
                        start_date=config['start_date'],
                        end_date=config['end_date'],
                        max_pages_per_day=config.get('max_pages_per_day', 50)
                    )
                
            except Exception as e:
                logging.error(f"çˆ¬å–ä»»åŠ¡å‡ºé”™: {e}")
        
        # æ‰§è¡Œä¸€æ¬¡
        crawl_task()
        
        # å¦‚æœæ˜¯è¿ç»­æ¨¡å¼ï¼Œè®¾ç½®å®šæ—¶ä»»åŠ¡
        if config.get('mode') == 'continuous':
            interval_hours = config.get('interval_hours', 6)
            schedule.every(interval_hours).hours.do(crawl_task)
            
            logging.info(f"å·²è®¾ç½®æ¯{interval_hours}å°æ—¶æ‰§è¡Œä¸€æ¬¡çˆ¬å–ä»»åŠ¡")
            
            while self.is_running:
                schedule.run_pending()
                time.sleep(60)
    
    def stop(self):
        """åœæ­¢çˆ¬è™«"""
        self.is_running = False
        if hasattr(self, 'driver'):
            try:
                self.driver.quit()
            except:
                pass
        logging.info("çˆ¬è™«å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    
    # ========== é…ç½®åŒºåŸŸ ==========
    # è¯·æ ¹æ®éœ€è¦ä¿®æ”¹ä»¥ä¸‹é…ç½®
    
    config = {
        # è¿è¡Œæ¨¡å¼: 'single'(å•æ¬¡è¿è¡Œ) æˆ– 'continuous'(24å°æ—¶ä¸é—´æ–­)
        'mode': 'single',
        
        # è¦æœç´¢çš„å…³é”®è¯åˆ—è¡¨
        'keywords': [
            #'ä¿®æ”¹ä¸ºè‡ªå·±å®é™…éœ€è¦ä½¿ç”¨çš„å…³é”®è¯',
        ],
        
        # å†å²æ•°æ®çˆ¬å–æ—¶é—´èŒƒå›´
        #'start_date': 'æ ¹æ®éœ€è¦ï¼Œé…ç½®æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ä¸ºï¼š2016-11-08',
        #'end_date': 'æ ¹æ®éœ€è¦ï¼Œé…ç½®æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ä¸ºï¼š2016-12-31',  
        
        # æ¯å¤©æœ€å¤šçˆ¬å–çš„é¡µæ•°
        'max_pages_per_day': 50, 
        
        # è¿ç»­æ¨¡å¼ä¸‹çš„çˆ¬å–é—´éš”ï¼ˆå°æ—¶ï¼‰
        'interval_hours': 2,
    }
    
    # ========== åˆ›å»ºçˆ¬è™«å®ä¾‹ ==========
    try:
        print("\næ­£åœ¨åˆå§‹åŒ–çˆ¬è™«...")
        print("è¯·æ³¨æ„ï¼šç¨‹åºå°†è‡ªåŠ¨æ‰“å¼€Chromeæµè§ˆå™¨è¿›è¡Œå¾®åšç™»å½•")
        print("è¯·åœ¨æ–°æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•æ“ä½œ")
        crawler = WeiboAdvancedSearchCrawler(config)
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # ========== æ­£å¸¸çˆ¬å–æµç¨‹ ==========
    
    print("=" * 60)
    print("å¾®åšå†å²æ•°æ®çˆ¬è™«ç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    print(f"æ•°æ®ä¿å­˜è·¯å¾„: {crawler.base_path}")
    print(f"è¿è¡Œæ¨¡å¼: {config['mode']}")
    print(f"æ—¶é—´èŒƒå›´: {config['start_date']} åˆ° {config['end_date']}")
    print(f"å…³é”®è¯: {', '.join(config['keywords'])}")
    print("å­˜å‚¨æ ¼å¼: æ¯ä¸ªå…³é”®è¯ä¸€ä¸ªExcelæ–‡ä»¶ï¼Œæ¯å¹´ä¸€ä¸ªå·¥ä½œè¡¨")
    print(f"ç™»å½•æ–¹å¼: Seleniumè‡ªåŠ¨åŒ–ç™»å½•")
    print("=" * 60)
    print()
    
    try:
        if config['mode'] == 'continuous':
            logging.info("å¯åŠ¨24å°æ—¶ä¸é—´æ–­å¾®åšçˆ¬è™«...")
        else:
            logging.info("å¯åŠ¨å†å²æ•°æ®çˆ¬å–...")
        
        crawler.start_continuous_crawling(config)
        
    except KeyboardInterrupt:
        print("\n" + "=" * 60)
        logging.info("æ¥æ”¶åˆ°åœæ­¢ä¿¡å·...")
        crawler.stop()
        print("=" * 60)
    except Exception as e:
        logging.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        crawler.stop()
    finally:
        print("\n" + "=" * 60)
        print(f"çˆ¬å–å®Œæˆç»Ÿè®¡")
        print(f"æ•°æ®æ€»æ•°: {crawler.total_weibo_count} æ¡")
        print("=" * 60)


if __name__ == "__main__":
    main()
